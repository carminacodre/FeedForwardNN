{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functionality for a Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self,activation):\n",
    "        self.activation = activation\n",
    "    def get_function(self):\n",
    "        def tanh(z):\n",
    "            return np.tanh(z)\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        def relu(z):\n",
    "            return np.maximum(z, 0)\n",
    "            return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        if (self.activation == 'tanh'):\n",
    "            return tanh\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return sigmoid\n",
    "        if (self.activation == 'relu'):\n",
    "            return relu\n",
    "    def get_derivative(self):\n",
    "        def tanh_derivative(output):\n",
    "            return 1.0 - output**2\n",
    "        def sigmoid_derivative(output):\n",
    "            return output*(1-output)\n",
    "        def relu_derivative(z):\n",
    "            result = np.empty_like(z)\n",
    "            result[z<=0] = 0\n",
    "            result[z>0] = 1\n",
    "            return result\n",
    "        if (self.activation == 'tanh'):\n",
    "            return tanh_derivative\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return sigmoid_derivative\n",
    "        if (self.activation == 'relu'):\n",
    "            return relu_derivative\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, units, activation):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim\n",
    "        np.random.seed(0)\n",
    "        self.weights = np.random.randn(self.input_dim, self.units)/ np.sqrt(self.input_dim)\n",
    "        self.biases = np.zeros((1,self.units))\n",
    "    def compute(self,X):\n",
    "        self.input = X.copy()\n",
    "        self.output = self.activation.get_function()(X.dot(self.weights)+self.biases)\n",
    "        return self.output\n",
    "    def print_info(self):\n",
    "        print(\"Layer with %d units and %s activation\" %(self.units,self.activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model2Layers:\n",
    "    def __init__(self,input_dim, output_dim, hidden_dim,  learning_rate=0.001, reg_lambda = 0.0005, \n",
    "                 activation_layer_1=\"sigmoid\", activation_layer_2 = \"sigmoid\"):\n",
    "        \n",
    "        self.synapse_0 = Layer(input_dim,hidden_dim, Activation(activation_layer_1))\n",
    "        self.synapse_1 = Layer(hidden_dim,output_dim, Activation(activation_layer_2))\n",
    "        \n",
    "        self.learning_rate = learning_rate     \n",
    "        \n",
    "        self.layers = [self.synapse_0, self.synapse_1]\n",
    "        \n",
    "        self.reg_lambda = reg_lambda\n",
    "    \n",
    "    def feed_forward(self,X):\n",
    "        \n",
    "        output = X.copy()\n",
    "        \n",
    "        for l in self.layers:\n",
    "            output = l.compute(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        out = self.feed_forward(x)\n",
    "        if out.shape[1] == 1:\n",
    "            return 1 if out[0][0]>0.5 else 0\n",
    "        else:\n",
    "            return np.argmax(out,axis= 1)[0]\n",
    "    \n",
    "    def compute_loss(self,output,y):\n",
    "        \n",
    "        error = output - y \n",
    "        return np.mean(error**2)\n",
    "    \n",
    "    def compute_accuracy(self,output,y):\n",
    "        \n",
    "        num_samples = y.shape[0]\n",
    "        \n",
    "        acc= float(np.sum(np.argmax(y, axis= 1) == np.argmax(output, axis= 1)))/num_samples\n",
    "        \n",
    "        return acc\n",
    "    def back_propagation(self, X, y, no_iterations = 60000):\n",
    "        \n",
    "        for j in xrange(no_iterations):\n",
    "            \n",
    "            # Feed forward through layers 0, 1, and 2            \n",
    "            output = self.feed_forward(X)\n",
    "        \n",
    "            #compute error\n",
    "            error_2 = output - y\n",
    "            \n",
    "            if (j% 10000) == 0:\n",
    "                print \"Error    after \"+str(j)+\" iterations:\" + str(self.compute_loss(output,y))\n",
    "                print \"Accuracy after \"+str(j)+\" iterations:\" + str(self.compute_accuracy(output,y))\n",
    "            \n",
    "            layer_2_delta = error_2*self.synapse_1.activation.get_derivative()(self.synapse_1.output)\n",
    "            \n",
    "            #backprop the error to layer 1\n",
    "            error_1 = layer_2_delta.dot(self.synapse_1.weights.T)\n",
    "            \n",
    "            layer_1_delta = error_1*self.synapse_0.activation.get_derivative()(self.synapse_0.output)\n",
    "\n",
    "            #update the weights\n",
    "            # Add regularization terms\n",
    "                \n",
    "            dW2 = self.synapse_1.input.T.dot(layer_2_delta)\n",
    "            dW1 = self.synapse_0.input.T.dot(layer_1_delta)\n",
    "            \n",
    "            dW2 += self.reg_lambda * self.synapse_1.weights\n",
    "            dW1 += self.reg_lambda * self.synapse_0.weights\n",
    "            \n",
    "            self.synapse_1.weights -= self.learning_rate * dW2\n",
    "            self.synapse_1.biases -= self.learning_rate * (np.ones((1,self.synapse_1.input.shape[0])).dot(layer_2_delta))\n",
    "            \n",
    "            self.synapse_0.weights -= self.learning_rate * dW1\n",
    "            self.synapse_0.biases -= self.learning_rate * (np.ones((1,self.synapse_0.input.shape[0])).dot(layer_1_delta))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
