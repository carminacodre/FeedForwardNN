{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functionality for a Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self,activation):\n",
    "        self.activation = activation\n",
    "    def get_function(self):\n",
    "        def tanh(z):\n",
    "            return np.tanh(z)\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        def relu(z):\n",
    "            return np.maximum(z, 0)\n",
    "        def softmax(z):\n",
    "            exp_z = np.exp(z)\n",
    "            return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        if (self.activation == 'tanh'):\n",
    "            return tanh\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return sigmoid\n",
    "        if (self.activation == 'relu'):\n",
    "            return relu\n",
    "        if (self.activation == 'softmax'):\n",
    "            return softmax\n",
    "    def get_derivative(self):\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        def tanh_derivative(z):\n",
    "            return 1.0 - np.tanh(z)**2\n",
    "        def sigmoid_derivative(output):\n",
    "            #return np.multiply(sigmoid(z), (1-sigmoid(z)))\n",
    "            return output*(1-output)\n",
    "        def relu_derivative(z):\n",
    "            result = np.empty_like(z)\n",
    "            result[z<=0] = 0\n",
    "            result[z>0] = 1\n",
    "            return result\n",
    "        if (self.activation == 'tanh'):\n",
    "            return tanh_derivative\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return sigmoid_derivative\n",
    "        if (self.activation == 'relu'):\n",
    "            return relu_derivative\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, units, activation):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim\n",
    "        np.random.seed(0)\n",
    "        self.weights = np.random.randn(self.input_dim, self.units)/ np.sqrt(self.input_dim)\n",
    "        self.biases = np.zeros((1,self.units))\n",
    "    def compute(self,X):\n",
    "        self.input = X.copy()\n",
    "        return self.activation.get_function()(X.dot(self.weights)+self.biases)\n",
    "    def print_info(self):\n",
    "        print(\"Layer with %d units and %s activation\" %(self.units,self.activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.77211124  0.60587342]\n",
      " [ 0.84070855  0.65797664]\n",
      " [ 0.87795018  0.68697731]\n",
      " [ 0.89702411  0.69872825]\n",
      " [ 0.90720057  0.70251222]]\n",
      "('loss', 0.40134198478360922)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "('loss', nan)\n",
      "[[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carmi\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:53: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\Users\\carmi\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:29: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\Users\\carmi\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, learning_rate=0.0001):\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    def feed_forward(self,X):\n",
    "        output = X.copy()\n",
    "        for l in self.layers:\n",
    "            output = l.compute(output)\n",
    "            \n",
    "        return output\n",
    "    def compute_loss(self,X,y):\n",
    "        output = self.feed_forward(X)\n",
    "        # Calculating the loss\n",
    "        num_samples = len(X)\n",
    "        y_idx = np.argmax(y, axis = 1)\n",
    "        logprobs = -np.log(output[range(num_samples), y_idx])\n",
    "        data_loss = np.sum(logprobs)\n",
    "        return 1./num_samples * data_loss\n",
    "    def back_propagation(self,X,y):\n",
    "        output = self.feed_forward(X)\n",
    "        num_samples = len(output)\n",
    "        \n",
    "        #compute error from output\n",
    "        # Backpropagation\n",
    "        #derivative of loss against the prediction function\n",
    "#         inv_output = 1/output\n",
    "#         y_idx = np.argmax(y, axis = 1)\n",
    "\n",
    "#         deriv_loss_f = np.zeros((1, y.shape[1]))\n",
    "#         for s in range(num_samples):\n",
    "#             inv_o = 1/output[s]\n",
    "#             y_cnt = y[s]\n",
    "#             deriv_loss_f += np.multiply(y_cnt, inv_o)\n",
    "        deriv_loss_f = np.zeros((1, y.shape[1]))\n",
    "        for s in range(num_samples):\n",
    "            deriv_loss_f += output[s] - y[s]\n",
    "            \n",
    "            \n",
    "        for s in range(num_samples):\n",
    "            for (l,i) in zip(reversed(self.layers), range(0,len(self.layers))):\n",
    "                # derrivative of the prediction function against z\n",
    "                z = l.input.dot(l.weights)+l.biases\n",
    "                deriv_f_z = l.activation.get_derivative()(z)\n",
    "                deriv_f_z = np.sum(deriv_f_z, axis=0, keepdims=True)\n",
    "\n",
    "                #derivative of z agains weights\n",
    "                # for weights the derivative is the input to the current layer\n",
    "                deriv_z_w = l.input\n",
    "                deriv_z_w = np.sum(deriv_z_w, axis=0, keepdims=True)\n",
    "\n",
    "                deriv_loss_z = np.multiply(deriv_loss_f, deriv_f_z)\n",
    "                deriv_loss_w =  deriv_z_w.T * deriv_loss_z  \n",
    "\n",
    "                # for biases the derivative is 1\n",
    "                deriv_loss_b = deriv_loss_z\n",
    "\n",
    "                # accumulate gradient for lower layers\n",
    "                deriv_z_x = l.weights.copy()\n",
    "                deriv_loss_f = deriv_loss_z.dot(l.weights.T)\n",
    "\n",
    "                #do the update \n",
    "                l.weights -= self.learning_rate * deriv_loss_w\n",
    "                l.biases -= self.learning_rate * deriv_loss_b  \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "y = np.array([[0, 1], [0, 1], [0, 1], [0, 1], [0, 1]])\n",
    "l1 = Layer(2, 5, Activation(\"sigmoid\"))\n",
    "l2 = Layer(5, 2, Activation(\"sigmoid\"))\n",
    "m = Model(learning_rate=1e-2)\n",
    "m.add_layer(l1)\n",
    "m.add_layer(l2)\n",
    "#m.feed_forward(X)\n",
    "print(m.feed_forward(X))\n",
    "\n",
    "print(\"loss\", m.compute_loss(X,y))\n",
    "\n",
    "for step in range(10):\n",
    "    m.back_propagation(X,y)\n",
    "    print(\"loss\", m.compute_loss(X, y))\n",
    "    \n",
    "print(m.feed_forward(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (2L, 3L)\n",
      "output shape: (5L, 3L)\n",
      "weights shape: (3L, 2L)\n",
      "output shape: (5L, 2L)\n",
      "weights shape: (2L, 3L)\n",
      "output shape: (5L, 3L)\n",
      "weights shape: (3L, 2L)\n",
      "output shape: (5L, 2L)\n",
      "('z.shape', (5L, 2L))\n",
      "('deriv_f_z.shape', (5L, 2L))\n",
      "('z.shape', (5L, 3L))\n",
      "('deriv_f_z.shape', (5L, 3L))\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(5,2)\n",
    "y = np.array([[0, 1],[0, 1],[0, 1],[1, 0],[1, 0]])\n",
    "l1 = Layer(2, 3, Activation(\"tanh\"))\n",
    "l2 = Layer(3, 2, Activation(\"sigmoid\"))\n",
    "m = Model()\n",
    "m.add_layer(l1)\n",
    "m.add_layer(l2)\n",
    "#m.feed_forward(X)\n",
    "m.compute_loss(X,y)\n",
    "m.back_propagation(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model2Layers:\n",
    "    def __init__(self,input_dim, output_dim, hidden_dim,  learning_rate=0.001):\n",
    "        self.synapse_0 = Layer(input_dim,hidden_dim, Activation(\"sigmoid\"))\n",
    "        self.synapse_1 = Layer(hidden_dim,output_dim, Activation(\"sigmoid\"))\n",
    "        self.learning_rate = learning_rate     \n",
    "    def feed_forward(self,X):\n",
    "        output = X.copy()\n",
    "        for l in self.layers:\n",
    "            output = l.compute(output)\n",
    "        return output\n",
    "    def compute_loss(self,X,y):\n",
    "        output = self.feed_forward(X)\n",
    "        # Calculating the loss\n",
    "        num_samples = len(X)\n",
    "        y_idx = np.argmax(y, axis = 1)\n",
    "        logprobs = -np.log(output[range(num_samples), y_idx])\n",
    "        data_loss = np.sum(logprobs)\n",
    "        return 1./num_samples * data_loss\n",
    "    def back_propagation(self,X,y):\n",
    "        for j in xrange(60000):\n",
    "            # Feed forward through layers 0, 1, and 2\n",
    "            out_0 = X\n",
    "            out_1 = self.synapse_0.activation.get_function()(np.dot(out_0,self.synapse_0.weights))\n",
    "            out_2 = self.synapse_1.activation.get_function()(np.dot(out_1,self.synapse_1.weights))             \n",
    "           \n",
    "            #compute error\n",
    "            error_2 = out_2 - y\n",
    "            \n",
    "            if (j% 10000) == 0:\n",
    "                print \"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(error_2)))\n",
    "            \n",
    "            layer_2_delta = error_2*self.synapse_1.activation.get_derivative()(out_2)\n",
    "            \n",
    "            #backprop the error to layer 1\n",
    "            error_1 = layer_2_delta.dot(self.synapse_1.weights.T)\n",
    "            \n",
    "            layer_1_delta = error_1*self.synapse_0.activation.get_derivative()(out_1)\n",
    "\n",
    "            #update the weights\n",
    "            \n",
    "            self.synapse_1.weights -= self.learning_rate * (out_1.T.dot(layer_2_delta))\n",
    "            self.synapse_0.weights -= self.learning_rate * (out_0.T.dot(layer_1_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after 0 iterations:0.500951768552\n",
      "Error after 10000 iterations:0.0128862545736\n",
      "Error after 20000 iterations:0.00834074944724\n",
      "Error after 30000 iterations:0.00651873219825\n",
      "Error after 40000 iterations:0.00548894456563\n",
      "Error after 50000 iterations:0.00481074237356\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])   \n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "m = Model2Layers(learning_rate = 0.8, input_dim= 3, output_dim=1, hidden_dim =5)\n",
    "\n",
    "m.back_propagation(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after 0 iterations:0.4514544711\n",
      "Error after 10000 iterations:0.18982039351\n",
      "Error after 20000 iterations:0.107813375289\n",
      "Error after 30000 iterations:0.129160249313\n",
      "Error after 40000 iterations:0.111567606928\n",
      "Error after 50000 iterations:0.106038873728\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "y = np.array([y]).T\n",
    "m = Model2Layers(learning_rate = 0.8, input_dim= 2, output_dim=1, hidden_dim =5)\n",
    "m.back_propagation(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
